{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import networkx\n",
    "import numpy\n",
    "from sklearn.preprocessing import normalize\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_remaining_self_loops, dense_to_sparse, from_networkx, to_dense_adj, to_networkx\n",
    "\n",
    "from data_utils import add_sparse, get_ppr_matrix_dense, get_random_data_split, normalize_adj\n",
    "from models.pushnet import PushNet\n",
    "from training import train_single_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "This notebooks demonstrates all steps necessary to train our models. As an example, we train *PushNet-PP* on the *CiteSeer* dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process dataset\n",
    "\n",
    "Our models require `torch_geometric.data.Data` objects as input containing:\n",
    "- A sparse propagation matrix (PPR-matrix or aggregated PPR-matrix) represented by\n",
    "    - **edge_index**: `torch.LongTensor` of shape *(2, num_nodes)* containing row and column indices\n",
    "    - **edge_weight**: `torch.FloatTensor` of size *num_nodes* containing the corresponding propagation weights\n",
    "- A dense node feature matrix\n",
    "    - **x**: `torch.FloatTensor` of shape *(num_nodes, num_dims)*\n",
    "- A label vector\n",
    "    - **y**: `torch.LongTensor` of size *num_nodes*\n",
    "    \n",
    "For each graph dataset, we perform the following pre-processing steps:\n",
    "- Restrict the graph to its largest connected component\n",
    "- Add all not already existing self-loops to the graph\n",
    "- Normalize the graph's adjacency matrix. We use symmetric normalization\n",
    "- *L1*-normalize all node feature vectors\n",
    "- Compute PPR-matrices for all $\\alpha$-values. We use a dense algorithm, since all graphs considered in the paper all small enough\n",
    "- Aggregate all PPR-matrices. We use *sum*-aggregation since it performed best and additionally allows for fast training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../data'\n",
    "dataset_name = 'Citeseer'\n",
    "\n",
    "# Load dataset\n",
    "dataset = Planetoid(root=data_root,\n",
    "                    name=dataset_name,\n",
    "                    transform=None,\n",
    "                    pre_transform=None,\n",
    "                    )\n",
    "\n",
    "# Get data\n",
    "data = dataset[0]\n",
    "edge_index = data.edge_index\n",
    "x = dataset.data.x.numpy().astype(numpy.float32)\n",
    "y = dataset.data.y\n",
    "\n",
    "# Restrict graph to largest connected component\n",
    "graph = to_networkx(data,\n",
    "                    to_undirected=False,\n",
    "                    )\n",
    "largest_cc = max(networkx.weakly_connected_components(graph), key=len)\n",
    "nodes_cc = numpy.sort(list(largest_cc))\n",
    "graph = graph.subgraph(largest_cc)\n",
    "data = from_networkx(graph)\n",
    "edge_index = data.edge_index\n",
    "x = x[nodes_cc, :]\n",
    "y = y[nodes_cc]\n",
    "\n",
    "# Add self-loops\n",
    "edge_index, edge_weight = add_remaining_self_loops(edge_index=edge_index,\n",
    "                                                   num_nodes=data.num_nodes,\n",
    "                                                   )\n",
    "\n",
    "# Normalize adjacency matrix\n",
    "edge_index, edge_weight = normalize_adj(edge_index=edge_index,\n",
    "                                        edge_weight=edge_weight,\n",
    "                                        num_nodes=data.num_nodes,\n",
    "                                        dtype=dataset.data.x.dtype,\n",
    "                                        normalization='sym',\n",
    "                                        )\n",
    "\n",
    "# Normalize features\n",
    "x = normalize(x,\n",
    "              norm='l1',\n",
    "              axis=1,\n",
    "              )\n",
    "x = torch.FloatTensor(x)\n",
    "\n",
    "# Compute PPR matrices\n",
    "adj = to_dense_adj(edge_index=edge_index,\n",
    "                   edge_attr=edge_weight,\n",
    "                  )[0].numpy()\n",
    "alphas = [0.05, 0.1, 0.2]\n",
    "epsilon = 1e-5\n",
    "edge_index = []\n",
    "edge_weight = []\n",
    "for alpha in alphas:\n",
    "    ppr_alpha = get_ppr_matrix_dense(adj=adj,\n",
    "                                     alpha=alpha,\n",
    "                                     epsilon=epsilon,\n",
    "                                    )\n",
    "    edge_index_alpha, edge_weight_alpha = dense_to_sparse(torch.FloatTensor(ppr_alpha))\n",
    "    edge_index += [edge_index_alpha]\n",
    "    edge_weight += [edge_weight_alpha]\n",
    "    \n",
    "# Perform sum aggregation\n",
    "edge_index, edge_weight = add_sparse(edge_index=edge_index,\n",
    "                                     edge_weight=edge_weight,\n",
    "                                    )\n",
    "\n",
    "# Prepare data object\n",
    "data = Data(edge_index=edge_index,\n",
    "            edge_attr=edge_weight,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2120\n",
      "Number of edges: 3679\n",
      "Number of features: 3703\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "# Print key statistics\n",
    "num_nodes = graph.number_of_nodes()\n",
    "num_edges = graph.number_of_edges() // 2  # undirected\n",
    "in_features = x.shape[1]\n",
    "num_classes = torch.unique(y).numel()\n",
    "print('Number of nodes: {}'.format(num_nodes))\n",
    "print('Number of edges: {}'.format(num_edges))\n",
    "print('Number of features: {}'.format(in_features))\n",
    "print('Number of classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "For simplicity, we train on a single random data split here.\n",
    "\n",
    "- Each split is defined by 3 binary masks indicating training, validation and test nodes. Each mask is represented by a `torch.BoolTensor` of size *num_nodes*. The data object needs to have the properties **train_mask**, **val_mask** and **test_mask** set before training.\n",
    "- For each split, we sample 20 nodes per class as training nodes. From the remaining nodes, we sample 500 nodes for validation. The remaining nodes are used for testing. All samples are drawn uniformly at random without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random data split\n",
    "split = get_random_data_split(data=data,\n",
    "                              num_classes=num_classes,\n",
    "                              num_train_per_class=20,\n",
    "                              num_val=500,\n",
    "                             )\n",
    "data.train_mask = split['train_mask']\n",
    "data.val_mask = split['val_mask']\n",
    "data.test_mask = split['test_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PushNet(in_features=in_features,\n",
    "                num_classes=num_classes,\n",
    "                variant='TPP',\n",
    "                dropout=0.5,\n",
    "                hidden_size=32,\n",
    "                bias=True,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU\n",
    "device = torch.device(0)\n",
    "data = data.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_acc, val_acc, test_acc, best_epoch, seconds_per_epoch = train_single_run(model=model,\n",
    "                                                                               data=data,\n",
    "                                                                               learning_rate=1e-2,\n",
    "                                                                               patience=100,\n",
    "                                                                               max_epochs=10000,\n",
    "                                                                               l2_reg=1e-2,\n",
    "                                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch after early stopping: 242\n",
      "Average training time per epoch: 0.0082s\n",
      "Training accuracy: 89.17%\n",
      "Validation accuracy: 75.20%\n",
      "Test accuracy: 75.33%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print('Best epoch after early stopping: {}'.format(best_epoch))\n",
    "print('Average training time per epoch: {:.4f}s'.format(seconds_per_epoch))\n",
    "print('Training accuracy: {:.2f}%'.format(train_acc * 100))\n",
    "print('Validation accuracy: {:.2f}%'.format(val_acc * 100))\n",
    "print('Test accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushnet",
   "language": "python",
   "name": "pushnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
